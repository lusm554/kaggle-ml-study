{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3522f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melbourne_data = pd.read_csv('../../data/melb_data.csv')\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Target\n",
    "y = filtered_melbourne_data.Price\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "            'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melbourne_data[features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split data into training and validation data, for both features and target\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8a007",
   "metadata": {},
   "source": [
    "## Experimenting With Different Models\n",
    "\n",
    "Now we can measure model accuracy. So, we can experiment with alternative models and seach best predictions.\n",
    "\n",
    "But how make `alternative model`? In docs of scikit-learn many options for decision tree model. The most important options determine the tree's depth. The tree's depth is a measure of how many splits it makes before coming to prediction.\n",
    "\n",
    "So, when we devide the houses amongst many leaves, we also have fewer houses in each leaf. It will make predictions that very close to real one, but on new data that predictions may be very unreliable. This phenomenon called `overfitting`, where a model matches the training datat almost perfectly, but poorly in validation and new data.\n",
    "\n",
    "On the other side if tree devided houses in 2 or 4 groups, resulting predictions may be far off for most houses, in training and validation data. When model fails to capture important distinctions and patterns in the data, that is called `underfitting`.\n",
    "\n",
    "<b>How solve this? </b> Since we care about accuracy on new data, which we estimate from our validadtion data, we want to find the balance between underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da3646",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. <br>\n",
    "But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "\n",
    "We can use a utility function to help compare MAE scores from different values for max_leaf_nodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03483f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes: int, train_X, valid_X, train_y, valid_y) -> int:\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(valid_X)\n",
    "    mae = mean_absolute_error(valid_y, predictions)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0be5a2",
   "metadata": {},
   "source": [
    "We can use for-loop to compare the accuracy of models built with different values for `max_leaf_nodes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbf9856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  347380\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  258171\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  254983\n",
      "Max leaf nodes: 7000  \t\t Mean Absolute Error:  254983\n",
      "Max leaf nodes: 10000  \t\t Mean Absolute Error:  254983\n",
      "CPU times: user 106 ms, sys: 2.87 ms, total: 109 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for max_leaf_nodes in [5, 50, 5_000, 7_000, 10_000]:\n",
    "    current_mae = get_mae(max_leaf_nodes, train_X, valid_X, train_y, valid_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, current_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfa3a9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Models can:\n",
    "- <b>Overfitting:<b/> capturing all wrong patterns that will not work in real data\n",
    "- <b>Underfitting:<b/> failing to capture relevants patterns because of low depth of tree.\n",
    "\n",
    "So, both `overfitting` and `underfitting` leading to less accurate predictions.\n",
    "    \n",
    "We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
