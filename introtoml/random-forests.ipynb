{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1e36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melbourne_data = pd.read_csv('../data/melb_data.csv')\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "y = melbourne_data.Price\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "            'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = melbourne_data[features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac4596",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data. <br>\n",
    "<b>So, find balance is hard.</b> But, many models have clever ideas that can lead to better predictions. Here we'll look at the `random forest` as an example.\n",
    "\n",
    "`How works random tree and why it's more accuracy?` The random forest uses many trees, making prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e3c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191669.7536453626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "predictions = forest_model.predict(valid_X)\n",
    "print(mean_absolute_error(valid_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff8a46",
   "metadata": {},
   "source": [
    "There more space for improvements. But this is already big improvement over the best decision tree error of 250,000. There are parameters which allows to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
